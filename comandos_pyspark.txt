import findspark
findspark.init()
import pyspark


from pyspark.sql import SQLContext


from pyspark import  SparkConf
from pyspark.sql.types import*
from pyspark.sql import SparkSession, HiveContext
import pandas as pd
import numpy as pt


from pyspark import SparkContext
sc =SparkContext()
sc.setLogLevel("WARN")


#weather1 = sc.textFile("C:/ariad/Weather/weather01.txt")
#weather2 = sc.textFile("C:/ariad/Weather/weather02.txt")
#weather3 = sc.textFile("C:/ariad/Weather/weather03.txt")
#weather4 = sc.textFile("C:/ariad/Weather/weather04.txt")
#weather5 = sc.textFile("C:/ariad/Weather/weather05.txt")
#weather6 = sc.textFile("C:/ariad/Weather/weather06.txt")


#weather1.collect()

#weather2.collect()

#weather3.collect()

#weather4.collect()


#weather5.collect()

#weather6.collect()

from functools import reduce  # For Python 3.x
from pyspark.sql import DataFrame

def unionAll(*dfs):
    return reduce(DataFrame.unionAll, dfs)

td = unionAll(*[weather1, weather2, weather3, weather4, weather5, weather6])